## 统计学习方法-朴素贝叶斯法
先提出以下问题:     
1. 朴素贝叶斯法、贝叶斯公式、贝叶斯估计分别是什么？
2. 贝叶斯公式的物理意义什么？
3. 贝叶斯网络是什么？

## 朴素贝叶斯法
朴素贝叶斯法 = 贝叶斯定理 + 特征条件独立. 
> 输入$X \in R^n$空间是n维向量集合，输出空间$y=\{c_1,c_2,...,c_K\}$. 所有的X和y都是对应空间上的随机变量. $P(X,Y)$是X和Y的联合概率分别. 训练数据集(由$P(X,Y)$独立同分布产生):
> $$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$ 
假设我们有这样的数据集怎么进行朴素贝叶斯算法学习呢？我们知道朴素贝叶斯法主要有两个条件组成,我们先看第一个**贝叶斯定理**:
1. 我们之前学到过的公式:
$$P(a|b)=\frac{P(a,b)}{P(b)}=\frac{P(b|a)P(a)}{\sum_{a \in A}P(b|a)P(a)} \tag{1}$$
2. 公式1是我们之前学到过的，下面我们基于机器学习法训练数据做一个变化:
   $$P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_{k}P(X=x|Y=c_k)P(Y=c_k)} \tag{2}$$

> 从公式2中我们已经看到了"后验概率最大化"，对于上述的$P(Y=c_k|X=x)$我们一般这样解释，当给定$(X=x)$的条件下，$Y=c_k$的概率，这就是条件概率. 这就简单了，我们只需要对于位置的x，计算其对应的$c_k,k \in [1,2,...,K]$的概率，选择最大的概率作为这个x的类别进行了. 

现在我们只要一定可以找到最好的结果了，这就开心了，下面就是怎么计算，路通了，怎么走就很简单了. 我们再看公式(2), 因为我们的x是向量形式，那么条件概率是:
$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_k), k=1,2,..,K \tag{3}$$

> 从公式3中，我们看到这里有"无数"的参数，很难进行直接估计计算，还记得我们提到的另外一个条件就是条件独立性的假设. 这里假设在给定$Y=c_k$的条件下，X的每一个特征维度是独立，而且这还是一个很强的假设. 因此我们有:
$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_k)=\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k) \tag{4}$$
 
我们将公式4带入公式2中，而且我们是比大小，并且分母是一样的，于是就得到了公式(5):
$$y=arg max_{c_k}P(Y=c_k)\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_k) \tag{5}$$

> 从公式(5)中，我们只需要计算出，$P(X^{(j)}=x^{(j)}|Y=c_k)$和$P(Y=c_k)$两个参数就行了.接下来就是基于数据进行参数计算，这就是贝叶斯法的参数估计. 

## 参数估计
一般使用极大似然估计法进行相应的概率估计. 先验概率$P(Y=c_k)$的极大似然估计是:
$$P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}, k=1,2,...,K \tag{6}$$

我们先设第j个特征$x^{(j)}$可能取值集合为$\{a_{j1},a_{j2},...,a_{js_j},\}$,条件概率的极大似然估计是:
$$P(X^{(j)}=x^{(j)}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x_i^j=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)} \tag{7}$$
$$j=1,2,...,n;l=1,2,..,S_j;k=1,2,...K$$
> 我们可以看到，基于数据估计的参数，将公式6和7带入公式5就得到了朴素贝叶斯分类器，本来我们是应该结束了，但是如何先验概率和条件概率，任何一个为0的话，整个估计的乘积就都是0了. 所有我们需要防止这个情况出现,就出现了**贝叶斯估计**.

## 贝叶斯估计
由于用极大似然进行估计的值，可能出现概率值为0的情况. 这才提出了使用贝叶斯估计，先验概率和条件概率分别对应的公式就是:
$$P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)+\lambda}{N+K \lambda}, k=1,2,...,K \tag{8}$$

$$P(X^{(j)}=x^{(j)}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x_i^j=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^{N}I(y_i=c_k)+S_j\lambda} \tag{9}$$

我们可以看出就是对每一个变量的多加了一个频数$\lambda \geq 0$. 当$\lambda = 0$时，就是极大似然估计. 通常取值$\lambda = 1$,这就是拉普拉斯平滑(Laplace smoothing). 显然这样的话所有的估计值都是正数，而且不会改变之前的序关系. 

## 回顾
1. 朴素贝叶斯法就是我们提到的这个分类器模型
2. 贝叶斯定理就是我们使用到公式(2)
3. 贝叶斯估计，就是防止出现0值的，参数估计法
4. 贝叶斯网络，就是输入变量之间存在概率依存关系，这是就是贝叶斯网络

再看贝叶斯公式, 它的目的是什么先验估计后验，并尽可能使后验最大化. 先验概率固定的时候，只能通过条件概率的值来改变后验概率. 看公式（10）和下图直接给出先验和后验的关系:
$$P(Y=c_k|X=x)=P(X=x|Y=c_k)P(Y=c_k) \tag{10}$$

<center><img src="https://i.ibb.co/zR0D5L7/Screenshot-from-2018-12-27-11-08-54.png" border="0"><br>先验和后验关系图</center>

## 总结
朴素贝叶斯法简单好用，计算量小，文本分类经常使用. 而且是贝叶斯网络的基础，还有就是强大的贝叶斯定理，它可以解释任何的机器学习的程度(可能是一句玩笑，哈哈).

**注: 生活如此，问题不大. 喵～**