## 统计学习方法笔记-感知机
> 感知机是一个二分类的监督模型，我们定义输入空间$x \in R^n$，每一个x都是用向量表示。输出空间$y=\{+1,-1\}$. 我们希望通过训练数据集合，学的权重参数w和偏置参数b，有:
> $$f(x) = sign(wx+b)$$
> sign函数是符号函数，表示如下：
> $$sign(x)=\begin{cases}
>   +1, x \geq 0 \\
>   -1, x < 0 
> \end{cases}  \tag{1}$$ 

> 感知机(perceptron)模型的目的就是找打一个超平面$wx+b=0$，将所有的训练数据分开，感知机模型要求输入数据**线性可分**。

## 学习模型
> 定义线性可分的数据，$T=\{(x_1,y_x),(x_2,y_2)...,(x_N,y_N) \}$,其中$x_i$是n维的实数向量，$y\in \{+1, -1\}$,如果找到一个平面$wx+b=0$将所有的数据正确分开，就返回这个平面。      
> 对于任何一个机器学习的模型，我们一定要找到一个损失函数，之后优化这个损失函数，最后得到这个算法模型的参数。我们选择损失函数的原则有，一可以表示这个模型的性能，二是能够方便优化。例如对于感知机我们要求最后没有误分类的点，我们可以选择最小化误分类的点，但是这个函数不是凸函数。于是我们选择最小化**所有误分类的点到超平面的距离**.(损失函数的选择是一个技术活)。点$x_0$到超平面S的距离表示：
> $$\frac{1}{||w||}|wx_0+b| \tag{2}$$

> 对于所有误分类的点$(x_i,y_i)$,我们知道有$wx_i+b<0$,由于$y_i \in \{+1, -1\}$, 所以对于误分类的点必有： $-y_i(wx_i+b)>0$。因此我们优化最小化所有误分类的点集合M，
> $$min_{w,b}L(w,b)=-\frac{1}{||w||}\sum_{x_i \in M}y_i(wx_i+b)=-\sum_{x_i \in M}y_i(wx_i+b) \tag{3}$$

> 使用梯度下降方法，我们知道有:
> $$\Delta_wL(w,b)=-\sum_{x_i \in M}y_ix_i \tag{4}$$
> $$\Delta_bL(w,b)=-\sum_{x_i \in M}y_i \tag{5}$$

> $$w_{t+1}=w_t-\eta\Delta_wL(w,b) \tag{6}$$
> $$b_{t+1}=b_t-\eta\Delta_bL(w,b) \tag{7}$$

> 其中$\eta(0<\eta \leq1)$是步长，又叫做学习率，表示参数更新的速度，建议不要太大，防止参数出现震荡，一般情况下学习率随着迭代次数逐渐减小。**注意**,我们计算损失函数的时候，没有加正则项，但是在实际的代码实现过程中，一定要加上正则项，一般使用L1和L2正则。


## 算法
> 输入:训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$,其中$x_i$是n维实数向量，$y_i\in\{+1, -1\}$,学习率$\eta(0<\eta \leq 1)$.     
输出: w,b, 感知机模型是$f(x)=sign(wx+b)$.
> 1. 选择初始值$w_0,b_0$
> 2. 遍历数据集，$(x_i,y_i)$
> 3. 如果$-y_i(wx_i+b)>0$,使用单个元素的梯度进行更新
>   $$w=w+\eta y_ix_i$$
>   $$b=b+\eta y_i$$
> 4. 如果存在，就直接转到2,知道所有的点都被正确分类。

算法的直观解释
> 算法的直观解释，就是如果发现一个实例点被误分类，就将分离超平面向错误的一侧进行移动，这样就可以减少误分类的点到超平面的距离（所有数据必须线性可分），直到所有点线性可分。从上面的计算中，我们可以看到我们每一轮(假设要T轮)，都有进行N次向量计算，我们知道向量计算是很浪费时间的，我们的时间计算复杂度有TN次向量计算，一般情况下，数据量都会很大，例如$T=100,000,N=10,000,000$,这个时候的我们训练模型是很慢的，以至于我们需要等待很长的时间。因此下面有一种对偶的解法，与此相对的，上面这种方法叫做原始解法，在支持向量机，我们会再次看到这种方法。


### 对偶形式
> 公式(4)-(7)，我们可以看到，最后的参数w和b，w是$y_ix_i$的线性组合，b是$y_i$的线性组合，对于每一次修改我们可以设置步长$\alpha_i$, 则有：
> $$w=\sum_{i=1}^{N}\alpha_i y_i x_i \tag{8}$$
> $$b=\sum_{i=1}^{N}\alpha_i y_i \tag{9}$$
> 这里，$\alpha_i \geq 0,i=1,2,...,N$, 表示第i实例点更新的个数，实例点更新的次数越多，表示它离超平面越近，越难正确分类。下面给出算法对偶形式，我们只需要用将w和b直接替换原始算法中的对应的更新公式即可。
> 输入:训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$,其中$x_i$是n维实数向量，$y_i\in\{+1, -1\}$,学习率$\eta(0<\eta \leq 1)$.$\alpha=(\alpha_1,\alpha_2,...,\alpha_N)$.     
输出: $\alpha$,b, 感知机模型是$f(x)=sign(\sum_{j=1}^{N}\alpha_j y_j x_j x+b)$.
> 1. 选择初始值$w_0,b_0$
> 2. 遍历数据集，$(x_i,y_i)$
> 3. 如果$-y_i(\sum_{j=1}^{N}\alpha_j y_j x_j x_i+b)>0$,使用单个元素的梯度进行更新， （这里的更新公式，你可以使用(8)替换w化简得到）
>   $$\alpha_i=\alpha_i + \eta$$ 
>   $$b=b+\eta y_i$$
> 
> 4. 如果存在，就直接转到2,知道所有的点都被正确分类。

刚才我们说过对对偶形式，比原始形式计算更快，这里我们可以看到，最浪费时间的向量相乘$(x_j ,x_i)$，我们可以直接一次计算好，存储起来，之后直接取出来即可，计算量大大降低。

## 总结
> 至此，感知机原理部分简单结束，下面就是开始我们的代码编写，还是老规矩，C++和Python。

> **生活如此，问题不大。喵～**